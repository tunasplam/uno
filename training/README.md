# Training

These agents need to train. In order to do this, we need to generate large training sets. Each will demonstrate valid moves.

```json
{
    "input": "prompt",
    "output": "response"
}
```

##  Autogenerated Train Data 

### Cases Covered

A script will generate samples of these scenarios below. Provide the agent the full prompt each time. Do not save the training set in git but instead save the script which generates and make sure that a seed allows for deterministic train set generates.

- Playing cards
  - Wild cards → change color (make sure you have that color in your hand)
  - Wild cards → same color (make sure you have that color in your hand)
  - Regular cards → same symbol
  - Regular cards → same color
- Drawing cards
  - When you need to (make sure to message agent)
  - When you do not need to
- Yell UNO
  - When you have one card in hand
  - When someone else has one card in hand

### Adjusting proportion of action types in train set.

You can pass custom ratios:

```python
ratios = {
    "draw_needed_forced": .10,
    "draw_needed_no_playable": .15,
    "draw_unneeded": .05,

    "play_regular_symbol": .25,
    "play_regular_color": .25,
    "play_wild": .1,

    "uno_defense": .05,
    "uno_offense": .05
}
```

## Supervised Fine-Tuning

The fine folks at huggingface already tuned our model, but on this step we use our training data to fine tune our models to play valid games of uno. Once this phase is done, these models should be able to play games of uno without ordeal. See `_config_training_args` for details on how this model works.

### TensorBoard

After starting the training pipeline, you can monitor its progress using the `tensorboard` dashboard. 

`uv run tensorboard --logdir /home/jordan/agents/uno/logs-{model_id}`

Then navigate to `http://localhost:6006/`.

## Reinforcement Learning

Once a model knows how to play uno, it plays uno in order to develop strategies.
